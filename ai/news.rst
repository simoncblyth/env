news about AI
================



* https://arstechnica.com/information-technology/2023/04/stable-diffusion-for-language-stability-launches-open-source-ai-chatbot/


* https://simonwillison.net/2023/May/4/no-moat/

* https://natural20.com/

May 5, 2023

* https://natural20.com/google-ai-documents-leak/


At the beginning of March the open source community got their hands on their
first really capable foundation model, as Meta’s LLaMA was leaked to the
public.

n many ways, this shouldn’t be a surprise to anyone. The current renaissance in
open source LLMs comes hot on the heels of a renaissance in image generation.
The similarities are not lost on the community, with many calling this the
“Stable Diffusion moment” for LLMs.

In both cases, low-cost public involvement was enabled by a vastly cheaper
mechanism for fine tuning called low rank adaptation, or LoRA, combined with a
significant breakthrough in scale (latent diffusion for image synthesis,
Chinchilla for LLMs).

* https://www.theguardian.com/technology/2023/may/05/google-engineer-open-source-technology-ai-openai-chatgpt

* https://www.semianalysis.com/p/google-we-have-no-moat-and-neither


Scalable Personal AI: You can finetune a personalized AI on your laptop in an evening.

* https://github.com/tloen/alpaca-lora
* https://huggingface.co/spaces/tloen/alpaca-lora
* https://github.com/tatsu-lab/stanford_alpaca





* https://medium.com/geekculture/list-of-open-sourced-fine-tuned-large-language-models-llm-8d95a2e0dc76


  

LLaMA-Adapter: Efficient Fine-tuning of Language Models
with Zero-init Attentio

* https://arxiv.org/pdf/2303.16199.pdf


* https://www.vice.com/en/article/xgwqgw/facebooks-powerful-large-language-model-leaks-online-4chan-llama




Large language models are having their Stable Diffusion moment

* https://simonwillison.net/2023/Mar/11/llama/


Stable Diffusion, ControlNet

* https://github.com/lllyasviel/ControlNet/blob/main/README.md



